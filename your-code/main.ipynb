{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (Á•ûÊ•ΩÂùÇË¶ö„ÄÖ)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "üòë<div class=\"col-md-6\">\n",
    "        <h1 class=\"h3 lh-condensed\">\n",
    "          <a data-hydro-click=\"{&quot;event_type&quot;:&quot;explore.click&quot;,&quot;payload&quot;:{&quot;click_context&quot;:&quot;TRENDING_DEVELOPERS_PAGE&quot;,&quot;click_target&quot;:&quot;OWNER&quot;,&quot;click_visual_representation&quot;:&quot;TRENDING_DEVELOPER&quot;,&quot;actor_id&quot;:73304043,&quot;record_id&quot;:661450,&quot;originating_url&quot;:&quot;https://github.com/trending/developers&quot;,&quot;user_id&quot;:73304043}}\" data-hydro-click-hmac=\"4aea39b2a13f130038af24b3638a067e342f3df020a870c0f82b26364cc9e8bc\" href=\"/arvidn\" data-view-component=\"true\">\n",
    "            Arvid Norberg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=soup.find_all(name=\"h1\",class_=\"h3 lh-condensed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trending developers\n",
      " ['Jonny Burger', 'Brandon', 'Lennart', 'Artur Arseniev', 'Remi Rousselet', 'Jeroen Ooms', 'Artem Zakharchenko', 'Kuitos', '‰∫åË¥ßÊú∫Âô®‰∫∫', 'Emiliano Heyns', 'shimat', 'Kristoffer Carlsson', 'J√©r√¥me Laban', 'Jonathan Dick', 'Dane Mackier', 'Ariel Mashraki', 'Felix Angelov', 'Rikki Schulte', 'David Rodr√≠guez', 'David Tolnay', 'Jason Quense', 'Nathan Rajlich', 'ÈôàÂ∏Ö', 'Arvid Norberg', 'David Anthoff']\n"
     ]
    }
   ],
   "source": [
    "name_clean=[]\n",
    "for n in names:\n",
    "    name_clean.append(n.getText().strip())\n",
    "print(\"Trending developers\\n\" , name_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_repositories = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "response_repositories = requests.get(url_repositories)\n",
    "soup = BeautifulSoup(response_repositories.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "repositories=soup.find_all(name=\"h1\",class_=\"h3 lh-condensed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['straightblast /\\n\\n      My-PoC-Exploits', 'bee-san /\\n\\n      pyWhat', 'plctlab /\\n\\n      v8-internals', 'trekhleb /\\n\\n      learn-python', 'PrefectHQ /\\n\\n      prefect', 'ansible /\\n\\n      awx', 'Azure /\\n\\n      azure-sdk-for-python', 'ifzhang /\\n\\n      FairMOT', 'wilsonfreitas /\\n\\n      awesome-quant', 'giswqs /\\n\\n      leafmap', 'chubin /\\n\\n      cheat.sh', 'kyclark /\\n\\n      tiny_python_projects', 'Hypdncy /\\n\\n      NessusToReport', 'microsoft /\\n\\n      restler-fuzzer', 'PaddlePaddle /\\n\\n      Research', 'edx /\\n\\n      edx-platform', 'tiangolo /\\n\\n      fastapi', 'fishtown-analytics /\\n\\n      dbt', 'deepinsight /\\n\\n      insightface', 'dmlc /\\n\\n      dgl', 'bentoml /\\n\\n      BentoML', 'heartexlabs /\\n\\n      label-studio', 'microsoft /\\n\\n      recommenders', 'ultralytics /\\n\\n      yolov5', 'open-mmlab /\\n\\n      mmpose']\n"
     ]
    }
   ],
   "source": [
    "repositories_clean=[]\n",
    "for r in repositories:\n",
    "    repositories_clean.append(r.getText().strip())\n",
    "print(repositories_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['straightblast :      My-PoC-Exploits',\n",
       " 'bee-san :      pyWhat',\n",
       " 'plctlab :      v8-internals',\n",
       " 'trekhleb :      learn-python',\n",
       " 'PrefectHQ :      prefect',\n",
       " 'ansible :      awx',\n",
       " 'Azure :      azure-sdk-for-python',\n",
       " 'ifzhang :      FairMOT',\n",
       " 'wilsonfreitas :      awesome-quant',\n",
       " 'giswqs :      leafmap',\n",
       " 'chubin :      cheat.sh',\n",
       " 'kyclark :      tiny_python_projects',\n",
       " 'Hypdncy :      NessusToReport',\n",
       " 'microsoft :      restler-fuzzer',\n",
       " 'PaddlePaddle :      Research',\n",
       " 'edx :      edx-platform',\n",
       " 'tiangolo :      fastapi',\n",
       " 'fishtown-analytics :      dbt',\n",
       " 'deepinsight :      insightface',\n",
       " 'dmlc :      dgl',\n",
       " 'bentoml :      BentoML',\n",
       " 'heartexlabs :      label-studio',\n",
       " 'microsoft :      recommenders',\n",
       " 'ultralytics :      yolov5',\n",
       " 'open-mmlab :      mmpose']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repositories_clean=[r.replace(\"/\\n\\n\",\":\") for r in repositories_clean]\n",
    "repositories_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['straightblast :My-PoC-Exploits', 'bee-san :pyWhat', 'plctlab :v8-internals', 'trekhleb :learn-python', 'PrefectHQ :prefect', 'ansible :awx', 'Azure :azure-sdk-for-python', 'ifzhang :FairMOT', 'wilsonfreitas :awesome-quant', 'giswqs :leafmap', 'chubin :cheat.sh', 'kyclark :tiny_python_projects', 'Hypdncy :NessusToReport', 'microsoft :restler-fuzzer', 'PaddlePaddle :Research', 'edx :edx-platform', 'tiangolo :fastapi', 'fishtown-analytics :dbt', 'deepinsight :insightface', 'dmlc :dgl', 'bentoml :BentoML', 'heartexlabs :label-studio', 'microsoft :recommenders', 'ultralytics :yolov5', 'open-mmlab :mmpose']\n"
     ]
    }
   ],
   "source": [
    "repositories_clean=[r.replace(\"  \",\"\") for r in repositories_clean]\n",
    "print(repositories_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url= 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_link = soup.find_all('a', {'class':'image'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/File:Walt_Disney_1946.JPG',\n",
       " '/wiki/File:Walt_Disney_1942_signature.svg',\n",
       " '/wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " '/wiki/File:Trolley_Troubles_poster.jpg',\n",
       " '/wiki/File:Steamboat-willie.jpg',\n",
       " '/wiki/File:Walt_Disney_1935.jpg',\n",
       " '/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " '/wiki/File:Disney_drawing_goofy.jpg',\n",
       " '/wiki/File:DisneySchiphol1951.jpg',\n",
       " '/wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '/wiki/File:Walt_disney_portrait_right.jpg',\n",
       " '/wiki/File:Walt_Disney_Grave.JPG',\n",
       " '/wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '/wiki/File:Disney_Display_Case.JPG',\n",
       " '/wiki/File:Disney1968.jpg',\n",
       " '/wiki/File:Disneyland_Resort_logo.svg',\n",
       " '/wiki/File:Animation_disc.svg',\n",
       " '/wiki/File:P_vip.svg',\n",
       " '/wiki/File:Magic_Kingdom_castle.jpg',\n",
       " '/wiki/File:Video-x-generic.svg',\n",
       " '/wiki/File:Flag_of_Los_Angeles_County,_California.svg',\n",
       " '/wiki/File:Blank_television_set.svg',\n",
       " '/wiki/File:Flag_of_the_United_States.svg']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_link_href=[l.get(\"href\") for l in images_link]\n",
    "images_link_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No me sale :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url_usa_code = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html=requests.get(url_usa_code).text\n",
    "soup=BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_changed=soup.find_all(\"div\",{\"class\":\"usctitlechanged\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_changed2=[a.getText().strip() for a in titles_changed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 10 - Armed Forces Ÿ≠']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_changed2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "url_earthquake=\"https://www.emsc-csem.org/Earthquake/\"\n",
    "html=requests.get(url_earthquake).text\n",
    "soup=BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes_soup=soup.find_all(\"tr\",{\"class\":\"ligne1 normal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earthquake2021-05-27\\xa0\\xa0\\xa010:05:17.823min ago19.24\\xa0N\\xa0\\xa0155.41\\xa0W\\xa0\\xa032Ml2.4\\xa0ISLAND OF HAWAII, HAWAII2021-05-27 10:10',\n",
       " 'Fearthquake2021-05-27\\xa0\\xa0\\xa009:43:58.844min ago47.54\\xa0N\\xa0\\xa08.18\\xa0E\\xa0\\xa00ML2.1\\xa0SWITZERLAND2021-05-27 09:48',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa008:55:45.21hr 32min ago38.13\\xa0N\\xa0\\xa036.90\\xa0E\\xa0\\xa00ML2.9\\xa0CENTRAL TURKEY2021-05-27 09:22',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa008:54:11.01hr 34min ago7.23\\xa0N\\xa0\\xa082.09\\xa0W\\xa0\\xa010 M4.0\\xa0SOUTH OF PANAMA2021-05-27 08:56',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa008:47:55.41hr 40min ago19.41\\xa0N\\xa0\\xa0155.35\\xa0W\\xa0\\xa08Ml2.4\\xa0ISLAND OF HAWAII, HAWAII2021-05-27 08:53',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa008:34:39.01hr 53min ago17.28\\xa0S\\xa0\\xa069.99\\xa0W\\xa0\\xa019 M4.0\\xa0SOUTHERN PERU2021-05-27 08:46',\n",
       " 'Fearthquake2021-05-27\\xa0\\xa0\\xa007:36:58.82hr 51min ago43.83\\xa0N\\xa0\\xa0148.15\\xa0E\\xa0\\xa040mb4.8\\xa0EAST OF KURIL ISLANDS2021-05-27 09:16',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa007:12:34.03hr 16min ago19.65\\xa0S\\xa0\\xa069.54\\xa0W\\xa0\\xa0108ML2.8\\xa0TARAPACA, CHILE2021-05-27 07:41',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa006:57:01.03hr 31min ago9.43\\xa0N\\xa0\\xa0125.78\\xa0E\\xa0\\xa0153 M3.3\\xa0MINDANAO, PHILIPPINES2021-05-27 07:15',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa006:53:30.73hr 35min ago19.24\\xa0N\\xa0\\xa0155.41\\xa0W\\xa0\\xa034Md2.2\\xa0ISLAND OF HAWAII, HAWAII2021-05-27 06:56',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa006:28:24.24hr 00min ago19.37\\xa0N\\xa0\\xa0155.43\\xa0W\\xa0\\xa09Ml2.6\\xa0ISLAND OF HAWAII, HAWAII2021-05-27 06:33',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa006:20:30.04hr 08min ago23.93\\xa0S\\xa0\\xa067.08\\xa0W\\xa0\\xa0162ML2.8\\xa0SALTA, ARGENTINA2021-05-27 06:50',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa005:59:46.44hr 28min ago42.41\\xa0N\\xa0\\xa08.07\\xa0W\\xa0\\xa06ML2.2\\xa0SPAIN2021-05-27 06:10',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa005:49:53.04hr 38min ago38.08\\xa0N\\xa0\\xa036.85\\xa0E\\xa0\\xa07ML2.8\\xa0CENTRAL TURKEY2021-05-27 07:03',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa005:32:15.04hr 56min ago13.23\\xa0N\\xa0\\xa0123.81\\xa0E\\xa0\\xa09 M3.3\\xa0LUZON, PHILIPPINES2021-05-27 06:35',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa005:04:03.05hr 24min ago24.10\\xa0S\\xa0\\xa067.55\\xa0W\\xa0\\xa0228ML2.5\\xa0ANTOFAGASTA, CHILE2021-05-27 05:25',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa004:59:06.65hr 29min ago35.68\\xa0N\\xa0\\xa03.43\\xa0W\\xa0\\xa010ML2.2\\xa0STRAIT OF GIBRALTAR2021-05-27 05:51',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa004:53:29.75hr 35min ago35.56\\xa0N\\xa0\\xa03.64\\xa0W\\xa0\\xa010ML2.6\\xa0STRAIT OF GIBRALTAR2021-05-27 04:58',\n",
       " 'earthquake2021-05-27\\xa0\\xa0\\xa004:48:13.85hr 40min ago35.58\\xa0N\\xa0\\xa03.62\\xa0W\\xa0\\xa010ML2.3\\xa0STRAIT OF GIBRALTAR2021-05-27 05:40']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_20_earthquakes_list=[a.getText().strip() for a in earthquakes_soup[0:19]]\n",
    "last_20_earthquakes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021-05-27', 'e2021-05-2', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', 'e2021-05-2', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27', '2021-05-27']\n"
     ]
    }
   ],
   "source": [
    "earthquake_dates = [x[10:20] for x in last_20_earthquakes_list]\n",
    "print(earthquake_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7.823min a', '58.844min ', '5.21hr 32m', '1.01hr 34m', '5.41hr 40m', '9.01hr 53m', '58.82hr 51', '4.03hr 16m', '1.03hr 31m', '0.73hr 35m', '4.24hr 00m', '0.04hr 08m', '6.44hr 28m', '3.04hr 38m', '5.04hr 56m', '3.05hr 24m', '6.65hr 29m', '9.75hr 35m', '3.85hr 40m']\n"
     ]
    }
   ],
   "source": [
    "earthquake_times = [x[30:40] for x in last_20_earthquakes_list]\n",
    "print(earthquake_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24 N  155.41 ', '.54 N  8.18 E', 'o38.13 N  36.', 'o7.23 N  82.0', 'o19.41 N  155', 'o17.28 S  69.', 'go43.83 N  14', 'o19.65 S  69.', 'o9.43 N  125.', 'o19.24 N  155', 'o19.37 N  155', 'o23.93 S  67.', 'o42.41 N  8.0', 'o38.08 N  36.', 'o13.23 N  123', 'o24.10 S  67.', 'o35.68 N  3.4', 'o35.56 N  3.6', 'o35.58 N  3.6']\n"
     ]
    }
   ],
   "source": [
    "earthquake_lat_long = [x[45:58] for x in last_20_earthquakes_list]\n",
    "earthquake_lat_long=[lat.replace(\"\\xa0\",\" \") for lat in earthquake_lat_long]\n",
    "print(earthquake_lat_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 20 Earthquake regions:\n",
      " ['ISLAND OF HAWAII, HAWAII', 'DODECANESE ISLANDS, GREECE', 'SWITZERLAND', 'OFF COAST OF COSTA RICA', 'CENTRAL TURKEY', 'PAKISTAN', 'SOUTH OF PANAMA', 'OFFSHORE VALPARAISO, CHILE', 'ISLAND OF HAWAII, HAWAII', 'SULAWESI, INDONESIA', 'SOUTHERN PERU', 'SOUTHERN SUMATRA, INDONESIA', 'EAST OF KURIL ISLANDS', \"OFF COAST OF O'HIGGINS, CHILE\", 'TARAPACA, CHILE', 'ISLAND OF HAWAII, HAWAII', 'MINDANAO, PHILIPPINES', 'CENTRAL PERU', 'ISLAND OF HAWAII, HAWAII', 'OFFSHORE EL SALVADOR', 'ISLAND OF HAWAII, HAWAII', 'ISLAND OF HAWAII, HAWAII', 'SALTA, ARGENTINA', 'HAWAII REGION, HAWAII', 'SPAIN', 'NEAR COAST OF NICARAGUA', 'CENTRAL TURKEY', 'SICILY, ITALY', 'LUZON, PHILIPPINES', 'OFFSHORE VALPARAISO, CHILE', 'ANTOFAGASTA, CHILE', 'NORTH ISLAND OF NEW ZEALAND', 'STRAIT OF GIBRALTAR', 'HAITI REGION', 'STRAIT OF GIBRALTAR', 'SALTA, ARGENTINA', 'STRAIT OF GIBRALTAR', 'ISLAND OF HAWAII, HAWAII', 'CANARY ISLANDS, SPAIN REGION', 'JAVA, INDONESIA', 'WESTERN TEXAS', 'VANUATU', 'WESTERN TURKEY', 'JAVA, INDONESIA', 'SPAIN', 'CANARY ISLANDS, SPAIN REGION', 'DOMINICAN REPUBLIC', 'CANARY ISLANDS, SPAIN REGION', 'NORTHERN ITALY', 'SPAIN']\n"
     ]
    }
   ],
   "source": [
    "regions = soup.find_all('td', {'class': 'tb_region'})\n",
    "print(\"Last 20 Earthquake regions:\\n\",[r.getText().strip() for r in regions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me qued√≥ muy mal el c√≥digo, regi√≥n no lo consegu√≠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "url_hack = 'https://hackevents.co/search/anything/anywhere/anytime' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_names=soup.find_all(\"div\",{'class':'central-featured-lang'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['English', '6', '299', '000+', 'articles'],\n",
       " ['Êó•Êú¨Ë™û', '1', '268', '000+', 'Ë®ò‰∫ã'],\n",
       " ['Espa√±ol', '1', '684', '000+', 'art√≠culos'],\n",
       " ['Deutsch', '2', '576', '000+', 'Artikel'],\n",
       " ['–†—É—Å—Å–∫–∏–π', '1', '724', '000+', '—Å—Ç–∞—Ç–µ–π'],\n",
       " ['Fran√ßais', '2', '329', '000+', 'articles'],\n",
       " ['Italiano', '1', '693', '000+', 'voci'],\n",
       " ['‰∏≠Êñá', '1', '197', '000+', 'Ê¢ùÁõÆ'],\n",
       " ['Portugu√™s', '1', '066', '000+', 'artigos'],\n",
       " ['Polski', '1', '473', '000+', 'hase≈Ç']]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_names_get=[lang.getText().split() for lang in language_names]\n",
    "language_names_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " 'Êó•Êú¨Ë™û',\n",
       " 'Espa√±ol',\n",
       " 'Deutsch',\n",
       " '–†—É—Å—Å–∫–∏–π',\n",
       " 'Fran√ßais',\n",
       " 'Italiano',\n",
       " '‰∏≠Êñá',\n",
       " 'Portugu√™s',\n",
       " 'Polski']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lang[0] for lang in language_names_get]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.gov.uk/'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = soup.find_all('h3', {'class': 'govuk-heading-s dgu-topics__heading'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business and economy', 'Crime and justice', 'Defence', 'Education', 'Environment', 'Government', 'Government spending', 'Health', 'Mapping', 'Society', 'Towns and cities', 'Transport', 'Digital service performance', 'Government reference data']\n"
     ]
    }
   ],
   "source": [
    "data_sets_names=[data.getText() for data in data_sets]\n",
    "print(data_sets_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
